{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b189d89",
   "metadata": {},
   "source": [
    "# Wildfire Disaster Predictor (Mini Dataset 500 Samples)\n",
    "Uses classification and regression with ML & DL (ANN, CNN, RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d68201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "from google.colab import files\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wildfire_risk_dataset_500.csv'\n",
    "if not os.path.exists(dataset_name):\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded:\n",
    "        if fn.endswith('.csv'):\n",
    "            dataset_name = fn\n",
    "\n",
    "df = pd.read_csv(dataset_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_activity = LabelEncoder()\n",
    "le_risk = LabelEncoder()\n",
    "df['Human Activity Level'] = le_activity.fit_transform(df['Human Activity Level'])\n",
    "df['Fire Risk Level'] = le_risk.fit_transform(df['Fire Risk Level'])\n",
    "\n",
    "X = df.drop('Fire Risk Level', axis=1)\n",
    "y_class = df['Fire Risk Level']\n",
    "y_reg = df['Drought Index']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X_scaled, y_class, test_size=0.2, random_state=42)\n",
    "_, _, y_train_reg, y_test_reg = train_test_split(X_scaled, y_reg, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train_class)\n",
    "print('Logistic Regression accuracy:', lr.score(X_test, y_test_class))\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train_class)\n",
    "print('Random Forest accuracy:', rf.score(X_test, y_test_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_class))\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "ann.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_ann = ann.fit(X_train, y_train_class, epochs=15, batch_size=16, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train_reg)\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "print('Linear Regression MSE:', mean_squared_error(y_test_reg, y_pred_lin))\n",
    "print('Linear Regression MAE:', mean_absolute_error(y_test_reg, y_pred_lin))\n",
    "\n",
    "dnn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "dnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "history_dnn = dnn.fit(X_train, y_train_reg, epochs=15, batch_size=16, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7461bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn = X_scaled.reshape(-1, X.shape[1], 1)\n",
    "cnn = Sequential([\n",
    "    Conv1D(32, 3, activation='relu', input_shape=(X.shape[1],1)),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_cnn = cnn.fit(X_cnn, y_class, epochs=15, batch_size=16, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn = X_scaled.reshape(-1, X.shape[1], 1)\n",
    "rnn = Sequential([\n",
    "    LSTM(32, input_shape=(X.shape[1],1)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_rnn = rnn.fit(X_rnn, y_class, epochs=15, batch_size=16, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ANN classification\n",
    "y_pred = np.argmax(ann.predict(X_test), axis=1)\n",
    "print('ANN classification report:')\n",
    "print(classification_report(y_test_class, y_pred))\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(y_test_class, y_pred), annot=True, fmt='d')\n",
    "plt.title('ANN Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Regression evaluation for DNN\n",
    "y_pred_dnn = dnn.predict(X_test).flatten()\n",
    "print('DNN Regression MSE:', mean_squared_error(y_test_reg, y_pred_dnn))\n",
    "print('DNN Regression MAE:', mean_absolute_error(y_test_reg, y_pred_dnn))\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure()\n",
    "plt.plot(history_ann.history['loss'], label='ann loss')\n",
    "plt.plot(history_ann.history['val_loss'], label='ann val_loss')\n",
    "plt.legend(); plt.title('ANN Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_dnn.history['loss'], label='dnn loss')\n",
    "plt.plot(history_dnn.history['val_loss'], label='dnn val_loss')\n",
    "plt.legend(); plt.title('DNN Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca366a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save('ann_classifier.h5')\n",
    "dnn.save('dnn_regressor.h5')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
